{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "from sqlalchemy import create_engine\n",
    "engine=create_engine('postgresql://sayeg:edema7-Warbled@127.0.0.1:5433/switch_mexico')\n",
    "#importing data from our tables\n",
    "df=pd.read_csv(\"OrganizedTables/HourlyLoadPerNode.csv\",index_col=range(4))\n",
    "dfp=pd.read_csv(\"OrganizedTables/LoadHighlightsPerNode.csv\",index_col=range(2),header=range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a new dataframe for Median and Peak day dates and values\n",
    "col=[df.columns.tolist(),['MedianDay','MedianDayValues','PeakDay','PeakDayValues']]\n",
    "indx=[range(2016,2031),range(1,13)]\n",
    "dfs=pd.DataFrame(index=pd.MultiIndex.from_product(indx),columns=pd.MultiIndex.from_product(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#searching for the median and peak days of every node at each year and month\n",
    "for k in df.columns.tolist():\n",
    "    for a in range(2016,2031):\n",
    "        for m in range(1,13):\n",
    "            zz=[]\n",
    "            for d in df.xs([a,m,1],level=[0,1,3])[k].index.tolist():\n",
    "                zz.append(df.xs([a,m,d],level=range(3))[k].mean())\n",
    "\n",
    "    \n",
    "            if len(zz)%2 ==1:\n",
    "                median = np.median(np.array(zz))\n",
    "                indx_median = zz.index(median)\n",
    "            else:\n",
    "                n=np.random.randint(len(zz))\n",
    "                median = np.median(zz[-n])\n",
    "                indx_median = zz.index(median)\n",
    "                if indx_median>= n:\n",
    "                    indx_median= indx_median+1\n",
    "            dfs.xs([a,m])[k,'MedianDay']=str(indx_median).zfill(2)\n",
    "            dfs.xs([a,m])[k,'MedianDayValues']=df.xs([a,m,indx_median],level=range(3))[k].tolist()\n",
    "            dfs.xs([a,m])[k,'PeakDay']=str(dfp.xs([a,m])[k,'PeakDay']).zfill(2)\n",
    "            dfs.xs([a,m])[k,'PeakDayValues']=df.xs([a,m,dfp.xs([a,m])[k,'PeakDay']],level=range(3))[k].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a new dataframe to export into the tab file used in SWITCH\n",
    "col=[\"load_area\",'hour','load_mw']\n",
    "indx=[df.columns.tolist(),range(2016,2031),range(1,13),range(2),range(12)]\n",
    "#the \"range(2)\" part of the multindex is for selecting peak day (0) and median day (1)\"\n",
    "export=pd.DataFrame(index=pd.MultiIndex.from_product(indx),columns =col)\n",
    "for k in df.columns.tolist():\n",
    "    for a in range(2016,2031):\n",
    "        for m in range(1,13):\n",
    "             for h in range(12):\n",
    "                export.xs([k,a,m,0,h])[\"load_area\"]=k\n",
    "                export.xs([k,a,m,1,h])[\"load_area\"]=k\n",
    "                export.xs([k,a,m,1,h])[\"hour\"]=\"{0}{1}{2}{3}\".format(a,str(m).zfill(2),str(dfs.xs([a,m])[k,'MedianDay']).zfill(2),str(h*2+1).zfill(2))\n",
    "                export.xs([k,a,m,1,h])[\"load_mw\"]=dfs.xs([a,m])[k,'MedianDayValues'][(h*2)+1]\n",
    "                lst=dfs.xs([a,m])[k,'PeakDayValues']\n",
    "                if lst.index(max(lst))%2==0:\n",
    "                    export.xs([k,a,m,0,h])[\"hour\"]=\"{0}{1}{2}{3}\".format(a,str(m).zfill(2),dfs.xs([a,m])[k,'PeakDay'][0:dfs.xs([a,m])[k,'PeakDay'].index(\".\")].zfill(2),str(h*2).zfill(2))\n",
    "                    export.xs([k,a,m,0,h])['load_mw']=lst[h*2]\n",
    "                else:\n",
    "                    export.xs([k,a,m,0,h])[\"hour\"]=\"{0}{1}{2}{3}\".format(a,str(m).zfill(2),dfs.xs([a,m])[k,'PeakDay'][0:dfs.xs([a,m])[k,'PeakDay'].index(\".\")].zfill(2),str(h*2).zfill(2))\n",
    "                    export.xs([k,a,m,0,h])['load_mw']=lst[h*2]\n",
    "                    \n",
    "                    \n",
    "export.index=export[\"load_area\"].tolist()\n",
    "export.index.name=\"load_area\"\n",
    "export=export.drop('load_area',axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export.to_csv('OrganizedTables/la_hourly_demand_high.csv')\n",
    "export.to_sql('la_hourly_demand_high',engine,schema='mexico',if_exists='replace',chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
